import cv2
import numpy as np
import mediapipe as mp
import tensorflow as tf
from tensorflow.keras.models import load_model
from collections import deque
from PIL import ImageFont, ImageDraw, Image

# âœ… Attention ë ˆì´ì–´ (trainable, dtype ì¸ì ë°›ëŠ” ë²„ì „)
class Attention(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super(Attention, self).__init__(**kwargs)
        self.W = tf.keras.layers.Dense(1)

    def call(self, inputs):
        score = tf.nn.tanh(self.W(inputs))
        weights = tf.nn.softmax(score, axis=1)
        context_vector = weights * inputs
        return tf.reduce_sum(context_vector, axis=1)

# âœ… ëª¨ë¸ ë¡œë“œ
print("âœ… ëª¨ë¸ ë¡œë“œ ì¤‘...")
model = load_model("best_lstm_model.keras", custom_objects={'Attention': Attention})
print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

# âœ… í´ë˜ìŠ¤ëª… (í•œê¸€ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸)
classes = [
    "ì•ˆë…•í•˜ì„¸ìš”", "ë­í•´ìš”", "ì˜ ì§€ëƒˆì–´ìš”?", "ê³ ë§ˆì›Œìš”", "ë¯¸ì•ˆí•´ìš”",
    "ê´œì°®ì•„ìš”", "ì¢‹ì•„ìš”", "ì‹«ì–´ìš”", "ì£¼ì„¸ìš”", "ì‚¬ë‘í•´ìš”",
    "ì¶•í•˜í•´ìš”", "ì•ˆë…•íˆ ê³„ì„¸ìš”", "ì•ˆë…•íˆ ê°€ì„¸ìš”", "ìƒì¼ ì¶•í•˜í•´ìš”", "ë³´ê³  ì‹¶ì–´ìš”",
    "ë°°ê³ íŒŒìš”", "ì¡¸ë ¤ìš”", "ì¶”ì›Œìš”", "ë”ì›Œìš”", "ë„ì™€ì£¼ì„¸ìš”",
    "í™”ì´íŒ…!", "ë©‹ì ¸ìš”", "ì¡°ì‹¬í•˜ì„¸ìš”", "ìˆ˜ê³ í–ˆì–´ìš”", "í–‰ë³µí•˜ì„¸ìš”",
    "ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”", "ì¡°ê¸ˆë§Œ ë”", "ì—¬ê¸° ìˆì–´ìš”", "ì €ê¸° ìˆì–´ìš”", "ë‹¤ì‹œ í•œ ë²ˆ ë§í•´ ì£¼ì„¸ìš”"
]

# âœ… í•œê¸€ ì¶œë ¥ í•¨ìˆ˜
def put_text_kor(img, text, org, font_path='C:/Windows/Fonts/malgun.ttf', font_size=30, color=(0, 255, 0)):
    img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    draw = ImageDraw.Draw(img_pil)
    font = ImageFont.truetype(font_path, font_size)
    draw.text(org, text, font=font, fill=color)
    return cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)

# âœ… í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ í•¨ìˆ˜ (pose+face+hands)
def extract_keypoints(results):
    keypoints = []
    # pose
    if results.pose_landmarks:
        for lm in results.pose_landmarks.landmark:
            keypoints.extend([lm.x, lm.y, lm.z, lm.visibility])
    else:
        keypoints.extend([0] * 33 * 4)
    # face
    if results.face_landmarks:
        for lm in results.face_landmarks.landmark:
            keypoints.extend([lm.x, lm.y, lm.z])
    else:
        keypoints.extend([0] * 468 * 3)
    # left hand
    if results.left_hand_landmarks:
        for lm in results.left_hand_landmarks.landmark:
            keypoints.extend([lm.x, lm.y, lm.z])
    else:
        keypoints.extend([0] * 21 * 3)
    # right hand
    if results.right_hand_landmarks:
        for lm in results.right_hand_landmarks.landmark:
            keypoints.extend([lm.x, lm.y, lm.z])
    else:
        keypoints.extend([0] * 21 * 3)
    return np.array(keypoints)

# âœ… ì¹´ë©”ë¼ ì—´ê¸°
cap = cv2.VideoCapture(0)
print("âœ… cap ê°ì²´ ìƒì„±ë¨!")

if not cap.isOpened():
    print("âŒ ì¹´ë©”ë¼ ì—´ ìˆ˜ ì—†ìŒ!")
    exit()
print("âœ… ì¹´ë©”ë¼ ì—´ê¸° ì„±ê³µ!")

# âœ… ì‹œí€€ìŠ¤ ì´ˆê¸°í™”
sequence = deque(maxlen=259)
print("âœ… ì‹œí€€ìŠ¤ ì´ˆê¸°í™”!")

mp_holistic = mp.solutions.holistic
mp_drawing = mp.solutions.drawing_utils

with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:
    print("âœ… MediaPipe Holistic ì¤€ë¹„ ì™„ë£Œ!")
    while True:
        ret, frame = cap.read()
        if not ret:
            print("âŒ í”„ë ˆì„ ì½ê¸° ì‹¤íŒ¨")
            break

        frame = cv2.flip(frame, 1)
        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = holistic.process(image)

        keypoints = extract_keypoints(results)
        sequence.append(keypoints)

        if len(sequence) == 259:
            try:
                input_data = np.expand_dims(sequence, axis=0)
                predictions = model.predict(input_data, verbose=0)
                prob = np.max(predictions)
                predicted_idx = np.argmax(predictions)
                predicted_sentence = classes[predicted_idx]
                text = f"{predicted_sentence} ({prob:.2f})"
                print("âœ… ì˜ˆì¸¡ ê²°ê³¼:", text)
                frame = put_text_kor(frame, text, (10, 30))
            except Exception as e:
                print("âŒ ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜:", e)

        # ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸°
        mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)
        mp_drawing.draw_landmarks(frame, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)
        mp_drawing.draw_landmarks(frame, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)

        cv2.imshow("Sign Language Real-time", frame)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            print("ğŸ›‘ í”„ë¡œê·¸ë¨ ì¢…ë£Œ")
            break

cap.release()
cv2.destroyAllWindows()
